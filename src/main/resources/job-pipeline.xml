<?xml version="1.0" encoding="UTF-8" ?>
<pipeline-def name="event-consolidation" description="This is the process for etl'ing event data" version="1.0.0">
    <settings>
        <singleSparkSession setting="true" />
        <globalViewAsLocal setting="true" />
    </settings>

    <variables>
        <variable name="iam_password" value="${db.events.password}" decryptionKeyFile="${application.cloud.key.uri}" />
        <variable name="process_date" value="${application.process_date}" />
        <variable name="staging_uri" value="file:///c:/temp/staging" />
    </variables>

    <aliases>
        <alias name="file" type="com.it21learning.etl.source.FileReader" />
        <alias name="flat" type="com.it21learning.etl.source.FlatReader" />
        <alias name="setting" type="com.it21learning.etl.setting.SparkConfSetter" />
        <alias name="sql" type="com.it21learning.etl.transform.SqlTransformer" />
    </aliases>

    <udf-registration>
        <register prefix="event_" type="com.it21learning.etl.EventUdfRegister" />
        <register prefix="user_" type="com.it21learning.etl.UserUdfRegister" />
    </udf-registration>

    <job include="./job.xml" />
    <job name="transform-user-events">
        <action name="load-users">
            <actor type="file">
                <properties>
                    <format>csv</format>
                    <options>
                        <header>false</header>
                        <delimiter>,</delimiter>
                        <quote>"</quote>
                        <timestampFormat>yyyy/MM/dd HH:mm:ss</timestampFormat>
                    </options>
                    <ddlSchemaString>user_id long, birth_year int, gender string, location string</ddlSchemaString>
                    <fileUri>${event.recommendation.data.users.file}</fileUri>
                </properties>
            </actor>
            <output-view name="users" global="true" />
        </action>
        <action name="load-events">
            <actor type="flat">
                <properties>
                    <fileUri>${event.recommendation.data.events.file}</fileUri>
                </properties>
            </actor>
            <output-view name="events_raw" global="false" />
        </action>
        <action name="transform-events">
            <actor type="sql">
                <properties>
                    <sqlString>
                        select
                        substr(row_value, 1, 12) as event_id,
                        substr(row_value, 13, 16) as event_time,
                        substr(row_value, 29, 12) as event_host,
                        substr(row_value, 41, 64) as event_location
                        from events_raw
                        where row_no not in (1, 2) and substr(row_value, 6, 5) != 'TFYKR'
                    </sqlString>
                </properties>
            </actor>
            <input-views>
                <view name="events_raw" />
            </input-views>
            <output-view name="events" global="true" />
        </action>
    </job>

    <debug-staging>
        <uri>${staging_uri}</uri>
        <actions>
            <action name="transform-events" />
            <action name="load-events" />
        </actions>
    </debug-staging>
</pipeline-def>