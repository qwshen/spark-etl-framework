<?xml version="1.0" encoding="UTF-8" ?>
<pipeline-def name="event-consolidation" description="This is the process for transforming event data" version="1.0.0">
    <settings>
        <singleSparkSession setting="false" />
        <globalViewAsLocal setting="true" />
    </settings>

    <variables>
        <variable name="process_date" value="${events.process_date}" />
        <variable name="staging_uri" value="file:///tmp/staging/events" />
        <variable name="file_stream_dir" value="file:///tmp/file_streaming_result" />
        <variable name="redis.password" value="${events.db.password}" decryptionKeyString="${application.security.decryption.key}" />
    </variables>

    <aliases>
        <alias name="redis-stream-reader" type="com.it21learning.etl.source.RedisStreamReader" />
        <alias name="redis-reader" type="com.it21learning.etl.source.RedisReader" />
        <alias name="sql" type="com.it21learning.etl.transform.SqlTransformer" />
        <alias name="file-stream-writer" type="com.it21learning.etl.sink.FileStreamWriter" />
    </aliases>

    <job name="prepare events-features">
        <action name="load users">
            <actor type="redis-stream-reader">
                <properties>
                    <host>localhost</host>
                    <port>6379</port>
                    <dbNum>3</dbNum>
                    <dbTable>users</dbTable>
                    <authPassword>${redis.password}</authPassword>
                    <options>
                        <key.pattern>users:*</key.pattern>
                        <stream.group.name>users-source</stream.group.name>
                        <stream.keys>user_id</stream.keys>
                        <stream.offsets>{ "offsets": { "user_id": { "groupName": "users-source", "offset": "0-0" } } }</stream.offsets>
                        <stream.read.batch.size>300</stream.read.batch.size>
                        <stream.read.block>1600</stream.read.block>
                    </options>
                    <ddlSchemaString>user_id string, birthyear string, gender string, joinedAt string</ddlSchemaString>
                    <addTimestamp>true</addTimestamp>
                    <watermark>
                        <timeField>__timestamp</timeField>
                        <delayThreshold>5 minutes</delayThreshold>
                    </watermark>
                </properties>
            </actor>
            <output-view name="users" />
        </action>
        <action name="load train">
            <actor type="redis-stream-reader">
                <properties>
                    <host>localhost</host>
                    <port>6379</port>
                    <dbNum>2</dbNum>
                    <dbTable>train</dbTable>
                    <authPassword>${redis.password}</authPassword>
                    <options>
                        <mode>binary</mode>
                        <key.pattern>train:*</key.pattern>
                        <stream.group.name>train-source</stream.group.name>
                        <stream.keys>train</stream.keys>
                        <stream.offsets>{ "offsets": { "train": { "groupName": "train-source", "offset": "0-0" } } }</stream.offsets>
                        <stream.read.batch.size>300</stream.read.batch.size>
                        <stream.read.block>1600</stream.read.block>
                    </options>
                    <dllSchemaString>user string, event long, timestamp string, interested int</dllSchemaString>
                    <addTimestamp>true</addTimestamp>
                    <watermark>
                        <timeField>__timestamp</timeField>
                        <delayThreshold>5 minutes</delayThreshold>
                    </watermark>
                </properties>
            </actor>
            <output-view name="train" />
        </action>
        <action name="transform users-train">
            <actor type="sql">
                <properties>
                    <sqlString>
                        select
                            u.user_id,
                            u.gender,
                            cast(u.birthyear as int) as birthyear,
                            t.timestamp,
                            cast(t.interested as int) as interested,
                            '${process_date}' as process_date
                        from train t
                            left join users u
                        on t.user = u.user_id and u.__timestamp &gt;= t.__timestamp - interval 60 seconds and u.__timestamp &lt;= t.__timestamp + interval 60 seconds
                    </sqlString>
                </properties>
            </actor>
            <input-views>
                <view name="users" />
                <view name="train" />
            </input-views>
            <output-view name="features" />
        </action>
        <action name="write features">
            <actor type="file-stream-writer">
                <properties>
                    <format>csv</format>
                    <options>
                        <header>true</header>
                        <maxRecordsPerFile>30000</maxRecordsPerFile>
                        <checkpointLocation>/tmp/redis_streaming_checkpoint</checkpointLocation>
                    </options>
                    <trigger>
                        <mode>processingTime</mode>
                        <interval>60 seconds</interval>
                    </trigger>
                    <partitionBy>interested</partitionBy>
                    <outputMode>append</outputMode>
                    <test>
                        <waittimeMS>16000</waittimeMS>
                    </test>
                    <fileUri>${file_stream_dir}</fileUri>
                    <view>features</view>
                </properties>
            </actor>
            <input-views>
                <view name="features" />
            </input-views>
        </action>
    </job>
</pipeline-def>