<?xml version="1.0" encoding="UTF-8" ?>
<etl-pipeline name="event-consolidation" description="This is the process for etl'ing event data" version="1.0.0">
    <settings>
        <singleSparkSession setting="false" />
        <globalViewAsLocal setting="true" />
    </settings>

    <variables>
        <variable name="process_date" value="${events.process_date}" />
        <variable name="staging_uri" value="file:///tmp/staging/events" />
        <variable name="export_dir" value="${events.output_dir}" />
    </variables>

    <aliases>
        <alias name="delta-stream-reader" type="com.it21learning.etl.source.DeltaStreamReader" />
        <alias name="sql" type="com.it21learning.etl.transform.SqlTransformer" />
        <alias name="kafka-stream-writer" type="com.it21learning.etl.sink.KafkaStreamWriter" />
    </aliases>

    <job name="prepare events-features">
        <action name="load features">
            <actor type="delta-stream-reader">
                <property name="options">
                    <option name="ignoreDeletes" value="true" />
                    <option name="ignoreChanges" value="true" />
                    <option name="startingVersion" value="1" />
                    <option name="maxBytesPerTrigger" value="4096" />
                    <option name="maxFilesPerTrigger" value="3" />
                </property>
                <property name="addTimestamp">true</property>
                <property name="waterMark">
                    <definition name="timeField">__timestamp</definition>
                    <definition name="delayThreshold">5 minutes</definition>
                </property>
                <property name="source">
                    <definition name="path">/tmp/delta</definition>
                </property>
            </actor>
            <output-view name="features" />
        </action>
        <action name="de-dup features">
            <actor type="sql">
                <property name="scriptFile">${application.scripts_uri}/de-duplicate-features.sql</property>
            </actor>
            <input-views>
                <view name="features" />
            </input-views>
            <output-view name="stream_features" />
        </action>
        <action name="write stream-features">
            <actor type="kafka-stream-writer">
                <property name="bootstrapServers">${kafka.bootstrap.servers}</property>
                <property name="topic">stream-features</property>
                <property name="options">
                    <option name="checkpointLocation" value="/tmp/checkpoint/distinct_features" />"
                </property>
                <property name="key">
                    <definition name="field">user_id</definition>
                </property>
                <property name="trigger">
                    <definition name="mode">processingTime</definition>
                    <definition name="interval">60 seconds</definition>
                </property>
                <property name="outputMode">complete</property>
                <property name="waitTimeInMs">9000</property>
                <property name="view">stream_features</property>
            </actor>
            <input-views>
                <view name="stream_features" />
            </input-views>
        </action>
    </job>

    <staging>
        <uri>${staging_uri}</uri>
        <actions all="false" />
    </staging>
</etl-pipeline>