<?xml version="1.0" encoding="UTF-8" ?>
<pipeline-def name="event-consolidation" description="This is the process for transforming event data" version="1.0.0">
    <settings>
        <singleSparkSession setting="false" />
        <globalViewAsLocal setting="true" />
    </settings>

    <variables>
        <variable name="process_date" value="${events.process_date}" />
        <variable name="staging_uri" value="file:///tmp/staging/events" />
        <variable name="export_dir" value="${events.output_dir}" />
    </variables>

    <aliases>
        <alias name="delta-stream-reader" type="com.it21learning.etl.source.DeltaStreamReader" />
        <alias name="sql" type="com.it21learning.etl.transform.SqlTransformer" />
        <alias name="kafka-stream-writer" type="com.it21learning.etl.sink.KafkaStreamWriter" />
    </aliases>

    <job name="prepare events-features">
        <action name="load features">
            <actor type="delta-stream-reader">
                <properties>
                    <options>
                        <ignoreDeletes>true</ignoreDeletes>
                        <ignoreChanges>true</ignoreChanges>
                        <startingVersion>1</startingVersion>
                        <maxBytesPerTrigger>4096</maxBytesPerTrigger>
                        <maxFilesPerTrigger>3</maxFilesPerTrigger>
                    </options>
                    <addTimestamp>true</addTimestamp>
                    <waterMark>
                        <timeField>__timestamp</timeField>
                        <delayThreshold>5 minutes</delayThreshold>
                    </waterMark>
                    <sourcePath>/tmp/delta</sourcePath>
                </properties>
            </actor>
            <output-view name="features" />
        </action>
        <action name="de-dup features">
            <actor type="sql">
                <properties>
                    <sqlFile>${application.scripts_uri}/de-duplicate-features.sql</sqlFile>
                </properties>
            </actor>
            <input-views>
                <view name="features" />
            </input-views>
            <output-view name="stream_features" />
        </action>
        <action name="write stream-features">
            <actor type="kafka-stream-writer">
                <properties>
                    <bootstrapServers>${kafka.bootstrap.servers}</bootstrapServers>
                    <topic>stream-features</topic>
                    <options>
                        <checkpointLocation>/tmp/checkpoint/distinct_features</checkpointLocation>
                    </options>
                    <keyField>user_id</keyField>
                    <trigger>
                        <mode>processingTime</mode>
                        <interval>60 seconds</interval>
                    </trigger>
                    <outputMode>complete</outputMode>
                    <test>
                        <waittimeMS>9000</waittimeMS>
                    </test>
                    <view>stream_features</view>
                </properties>
            </actor>
            <input-views>
                <view name="stream_features" />
            </input-views>
        </action>
    </job>
</pipeline-def>