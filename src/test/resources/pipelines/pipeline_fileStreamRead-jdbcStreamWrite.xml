 <?xml version="1.0" encoding="UTF-8" ?>
<etl-pipeline name="event-consolidation" description="This is the process for etl'ing event data" version="1.0.0">
    <settings>
        <singleSparkSession setting="false" />
        <globalViewAsLocal setting="true" />
    </settings>

    <variables>
        <variable name="process_date" value="${events.process_date}" />
        <variable name="staging_uri" value="file:///tmp/staging/events" />
        <variable name="db.password" value="${events.db.password}" decryptionKeyString="${application.security.decryption.key}" />
    </variables>

    <aliases>
        <alias name="file-stream-reader" type="com.it21learning.etl.source.FileStreamReader" />
        <alias name="sql" type="com.it21learning.etl.transform.SqlTransformer" />
        <alias name="jdbc-stream-writer" type="com.it21learning.etl.sink.JdbcStreamWriter" />
    </aliases>

    <job name="prepare events-features">
        <action name="load users">
            <actor type="file-stream-reader">
                <property name="format">csv</property>
                <property name="options">
                    <option name="header" value="true" />
                    <option name="delimiter" value="," />
                    <option name="maxFileAge" value="16h" />
                </property>
                <property name="ddlSchemaString">user_id string, locale string, birthyear string, gender string, joinedAt string, location string, timezone string</property>
                <property name="waterMark">
                    <definition name="timeField">__timestamp</definition>
                    <definition name="delayThreshold">5 minutes</definition>
                </property>
                <property name="addTimestamp">true</property>
                <property name="path">${events.users_input}</property>
            </actor>
            <output-view name="users" />
        </action>
        <action name="load train">
            <actor type="file-stream-reader">
                <property name="format">csv</property>
                <property name="options">
                    <option name="header" value="true" />
                    <option name="delimiter" value="," />
                    <option name="maxFileAge" value="9h" />
                </property>
                <property name="ddlSchemaString">user string, event string, invited string, timestamp string, interested string, not_interested string</property>
                <property name="waterMark">
                    <definition name="timeField">__timestamp</definition>
                    <definition name="delayThreshold">5 minutes</definition>
                </property>
                <property name="addTimestamp">true</property>
                <property name="path">${events.train_input}</property>
            </actor>
            <output-view name="train" />
        </action>
        <action name="transform users-train">
            <actor type="sql">
                <property name="scriptFile">${application.scripts_uri}/stream-user-train.sql</property>
            </actor>
            <input-views>
                <view name="users" />
                <view name="train" />
            </input-views>
            <output-view name="features" />
        </action>
        <action name="write features">
            <actor type="jdbc-stream-writer">
                <property name="db">
                  <definition name="driver">com.mysql.jdbc.Driver</definition>
                  <definition name="url">jdbc:mysql://localhost:3306/events</definition>
                  <definition name="tableName">features</definition>
                  <definition name="user">root</definition>
                  <definition name="password">${db.password}</definition>
                </property>
                <property name="dbOptions">
                  <option name="numPartitions" value="9" />
                  <option name="batchSize" value="1600" />
                  <option name="isolationlevel" value="READ_UNCOMMITTED" />
                </property>
                <property name="sinkStatement">
                  <definition name="sqlString">
                      insert into features(user_id, gender, birthyear, timestamp, invited, interested, process_date)
                      values(@user_id, @gender, @birthyear, @timestamp, @invited, @interested, @process_date)
                      on duplicate key update
                      gender = @gender, birthyear = @birthyear, timestamp = @timestamp, invited = @invited, interested = @interested
                  </definition>
                </property>
                <property name="options">
                  <option name="checkpointLocation" value="/tmp/checkpoint-continuous-staging" />
                </property>
                <property name="trigger">
                  <definition name="mode">continuous</definition>
                  <definition name="interval">30 seconds</definition>
                </property>
                <property name="outputMode">append</property>
                <property name="waitTimeInMs">60000</property>
                <property name="view">features</property>
            </actor>
            <input-views>
                <view name="features" />
            </input-views>
        </action>
    </job>
</etl-pipeline>