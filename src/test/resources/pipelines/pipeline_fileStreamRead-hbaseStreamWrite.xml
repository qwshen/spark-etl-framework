<?xml version="1.0" encoding="UTF-8" ?>
<pipeline-def name="event-consolidation" description="This is the process for transforming event data" version="1.0.0">
    <settings>
        <singleSparkSession setting="false" />
        <globalViewAsLocal setting="true" />
    </settings>

    <variables>
        <variable name="process_date" value="${events.process_date}" />
        <variable name="staging_uri" value="file:///tmp/staging/events" />
    </variables>

    <aliases>
        <alias name="file-stream-reader" type="com.qwshen.etl.source.FileStreamReader" />
        <alias name="flat-stram-reader" type="com.qwshen.etl.source.FlatStreamReader" />
        <alias name="sql" type="com.qwshen.etl.transform.SqlTransformer" />
        <alias name="hbase-stream-writer" type="com.qwshen.etl.sink.HBaseStreamWriter" />
    </aliases>

    <job name="prepare events-features">
        <action name="load users">
            <actor type="file-stream-reader">
                <properties>
                    <format>csv</format>
                    <options>
                        <header>true</header>
                        <delimiter>,</delimiter>
                        <maxFileAge>16h</maxFileAge>
                    </options>
                    <ddlSchemaString>user_id string, birthyear string, gender string, joinedAt string</ddlSchemaString>
                    <watermark>
                        <timeField>__timestamp</timeField>
                        <delayThreshold>5 minutes</delayThreshold>
                    </watermark>
                    <addTimestamp>true</addTimestamp>
                    <fileUri>${events.users_input}</fileUri>
                </properties>
            </actor>
            <output-view name="users" />
        </action>
        <action name="load train">
            <actor type="flat-stream-reader">
                <properties>
                    <options>
                        <maxFilesPerTrigger>9</maxFilesPerTrigger>
                    </options>
                    <row>
                        <valueField>value</valueField>
                    </row>
                    <watermark>
                        <timeField>__timestamp</timeField>
                        <delayThreshold>5 minutes</delayThreshold>
                    </watermark>
                    <addTimestamp>true</addTimestamp>
                    <fileUri>${events.train_input}</fileUri>
                </properties>
            </actor>
            <output-view name="train" />
        </action>
        <action name="transform users-train">
            <actor type="sql">
                <properties>
                    <sqlFile>${application.scripts_uri}/stream-user-train.sql</sqlFile>
                </properties>
            </actor>
            <input-views>
                <view name="users" />
                <view name="train" />
            </input-views>
            <output-view name="features" />
        </action>
        <action name="write features">
            <actor type="stream-stream-writer">
                <properties>
                    <connection>
                        <hbase.zookeeper.quorum>127.0.0.1</hbase.zookeeper.quorum>
                        <hbase.zookeeper.property.clientPort>2181</hbase.zookeeper.property.clientPort>
                        <zookeeper.znode.parent>/hbase-unsecure</zookeeper.znode.parent>
                    </connection>
                    <table>events_db:features</table>
                    <fieldsMapping>
                        <user_id>profile:users_id</user_id>
                        <gender>profile:gender</gender>
                        <birthyear>profile:birth_year</birthyear>
                        <interested>data:interested</interested>
                        <timestamp>data:time_stamp</timestamp>
                        <process_date>data:process_date</process_date>
                    </fieldsMapping>
                    <options>
                        <numPartitions>16</numPartitions>
                        <batchSize>1600</batchSize>
                        <checkpointLocation>/tmp/checkpoint-continuous-hbase</checkpointLocation>
                    </options>
                    <trigger>
                        <mode>processingTime</mode>
                        <interval>30 seconds</interval>
                    </trigger>
                    <outputMode>append</outputMode>
                    <test>
                        <waittimeMS>60000</waittimeMS>
                    </test>
                    <view>features</view>
                </properties>
            </actor>
            <input-views>
                <view name="features" />
            </input-views>
        </action>
    </job>
</pipeline-def>