pipeline-def:
  name: event-consolidation
  description: This is the process for transforming event data
  version: 1.0.0
  settings:
    singleSparkSession: false
    globalViewAsLocal: true
  variables:
    - name: process_date
      value: ${events.process_date}
    - name: staging_uri
      value: "file:///tmp/staging/events"
    - name: export_dir
      value: ${events.output_dir}
    - name: db.password
      value: ${events.db.password}
      decryptionKeyString: ${application.security.decryption.key}
  aliases:
    - name: flat-reader
      type: com.it21learning.etl.source.FlatReader
    - name: sql
      type: com.it21learning.etl.transform.SqlTransformer
    - name: jdbc-writer
      type: com.it21learning.etl.sink.JdbcWriter
  jobs:
    - name: prepare events-features
      actions:
        - name: load train
          actor:
            type: flat-reader
            properties:
              ddlFieldsString: "user:1-9 string, event:10-10 long, timestamp:20-32 string, interested:52-1 int"
              addInputFile: false
              fileUri: ${events.train_input}
          output-view:
            name: train
        - name: write train
          actor:
            type: jdbc-writer
            properties:
              connection:
                driver: com.mysql.jdbc.Driver
                url: jdbc:mysql://localhost:3306/mysql
                dbtable: train
                user: root
                password: ${db.password}
              mode: overwrite
              view: train
          input-view: [train]
