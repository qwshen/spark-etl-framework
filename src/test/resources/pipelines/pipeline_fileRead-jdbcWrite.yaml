pipeline-def:
  name: event-consolidation
  description: This is the process for transforming event data
  version: 1.0.0
  settings:
    singleSparkSession: false
    globalViewAsLocal: true
  variables:
    - name: process_date
      value: ${events.process_date}
    - name: staging_uri
      value: "file:///tmp/staging/events"
    - name: export_dir
      value: ${events.output_dir}
    - name: db.password
      value: ${events.db.password}
      decryptionKeyString: ${application.security.decryption.key}
  aliases:
    - name: flat-reader
      type: com.qwshen.etl.source.FlatReader
    - name: sql
      type: com.qwshen.etl.transform.SqlTransformer
    - name: jdbc-writer
      type: com.qwshen.etl.sink.JdbcWriter
  jobs:
    - name: prepare events-features
      actions:
        - name: load train
          actor:
            type: flat-reader
            properties:
              ddlFieldsString: "user:1-9 string, event:10-10 long, timestamp:20-32 string, interested:52-1 int"
              addInputFile: false
              fileUri: ${events.train_input}
          output-view:
            name: train
        - name: write train
          actor:
            type: jdbc-writer
            properties:
              connection:
                driver: com.mysql.jdbc.Driver
                url: jdbc:mysql://localhost:3306/mysql
                dbtable: train
                user: root
                password: ${db.password}
              options:
                truncate: true
              mode: overwrite
              view: train
          input-view: [train]
        - name: transform train
          actor:
            type: sql
            properties:
              sqlString: select user, event, timestamp, case when interested = 1 then 0 else 1 end as interested from train
          output-view:
            name: r_train
        - name: write transformed train
          actor:
            type: jdbc-writer
            properties:
              connection:
                driver: com.mysql.jdbc.Driver
                url: jdbc:mysql://localhost:3306/mysql
                dbtable: train
                user: root
                password: ${db.password}
              mode: append
              view: r_train
          input-view: [r_train]
        - name: update interested
          actor:
            type: sql
            properties:
              sqlString: select user, event, '2021-09-26 11:11:11' as timestamp, interested from r_train
          output-view:
            name: tr_train
        - name: write updated train
          actor:
            type: jdbc-writer
            properties:
              connection:
                driver: com.mysql.jdbc.Driver
                url: jdbc:mysql://localhost:3306/mysql
                dbtable: train
                user: root
                password: ${db.password}
              mode: merge
              sink:
                sqlString: >
                  insert into train(user, event, timestamp, interested) values(@user, @event, @timestamp, @interested) on duplicate key update timestamp = @timestamp
              view: tr_train
          input-view: [tr_train]

